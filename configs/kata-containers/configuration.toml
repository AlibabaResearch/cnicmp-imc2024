# Copyright (c) 2017-2019 Intel Corporation
#
# SPDX-License-Identifier: Apache-2.0
#

# XXX: WARNING: this file is auto-generated.
# XXX:
# XXX: Source file: "cli/config/configuration-qemu.toml.in"
# XXX: Project:
# XXX:   Name: Kata Containers
# XXX:   Type: kata

[hypervisor.dragonball]
path = "/usr/local/bin/dragonball"
kernel = "/usr/local/share/kangaroo2/vmlinux.alikernel"
image = "/usr/local/share/kangaroo2/rootfs.img.clearlinux"

# share ro sections
# share_ro_sections=true

# Optional space-separated list of options to pass to the guest kernel.
# For example, use `kernel_params = "vsyscall=emulate"` if you are having
# trouble running pre-2.15 glibc.
#
# WARNING: - any parameter specified here will take priority over the default
# parameter value of the same name used to start the virtual machine.
# Do not set values here unless you understand the impact of doing so as you
# may stop the virtual machine from booting.
# To see the list of default parameters, enable hypervisor debug, create a
# container and look for 'default-kernel-parameters' log entries.
kernel_params = "console=ttyS1 agent.log_vport=1025 agent.hotplug_timeout=30 vsyscall=emulate ipv6.disable=1"

# Machine KVM MMU EPT cache size
# For example: machine_kvmShadowMemory=2147479552
# machine_kvmShadowMemory=2147479552
# machine_ReserveForQemuMemory=2147483648
# Default number of vCPUs per SB/VM:
# unspecified or 0                --> will be set to 1
# < 0                             --> will be set to the actual number of physical cores
# > 0 <= number of physical cores --> will be set to the specified number
# > number of physical cores      --> will be set to the actual number of physical cores
default_vcpus = 1

# Default maximum number of vCPUs per SB/VM:
# unspecified or == 0             --> will be set to the actual number of physical cores or to the maximum number
#                                     of vCPUs supported by KVM if that number is exceeded
# > 0 <= number of physical cores --> will be set to the specified number
# > number of physical cores      --> will be set to the actual number of physical cores or to the maximum number
#                                     of vCPUs supported by KVM if that number is exceeded
# WARNING: Depending of the architecture, the maximum number of vCPUs supported by KVM is used when
# the actual number of physical cores is greater than it.
# WARNING: Be aware that this value impacts the virtual machine's memory footprint and CPU
# the hotplug functionality. For example, `default_maxvcpus = 240` specifies that until 240 vCPUs
# can be added to a SB/VM, but the memory footprint will be big. Another example, with
# `default_maxvcpus = 8` the memory footprint will be small, but 8 will be the maximum number of
# vCPUs supported by the SB/VM. In general, we recommend that you do not edit this variable,
# unless you know what are you doing.
default_maxvcpus = 0

# Bridges can be used to hot plug devices.
# Limitations:
# * Currently only pci bridges are supported
# * Until 30 devices per bridge can be hot plugged.
# * Until 5 PCI bridges can be cold plugged per VM.
#   This limitation could be a bug in qemu or in the kernel
# Default number of bridges per SB/VM:
# unspecified or 0   --> will be set to 1
# > 1 <= 5           --> will be set to the specified number
# > 5                --> will be set to 5
default_bridges = 1

# Default memory size in MiB for SB/VM.
# If unspecified then it will be set 2048 MiB.
default_memory = 512
#
# Default memory slots per SB/VM.
# If unspecified then it will be set 10.
# This is will determine the times that memory will be hotadded to sandbox/VM.
#memory_slots = 10

# The size in MiB will be plused to max memory of hypervisor.
# It is the memory address space for the NVDIMM devie.
# If set block storage driver (block_device_driver) to "nvdimm",
# should set memory_offset to the size of block device.
# Default 0
#memory_offset = 0

# Disable block device from being used for a container's rootfs.
# In case of a storage driver like devicemapper where a container's 
# root file system is backed by a block device, the block device is passed
# directly to the hypervisor for performance reasons. 
# This flag prevents the block device from being passed to the hypervisor, 
# 9pfs is used instead to pass the rootfs.
disable_block_device_use = false

# Enable KVM DISABLE_IDLE_EXITS feature
# enable_cpu_pm = true

# Shared file system type:
#   - virtio-9p (default)
#   - virtio-fs
shared_fs = "inline-virtio-fs"

# Path to vhost-user-fs daemon.
virtio_fs_daemon = "/usr/local/bin/virtiofsd2"

# Default size of DAX cache in MiB
#virtio_fs_cache_size = 1024

# Strategies for using dax:
#
#  - inode
#    Turn on the per-file dax function, whether a file is accessed through dax
#    is controlled by a special policy.
#
#  - always
#    All files are accessible via dax.
#
#  - never
#    All files are not accessible via dax
#
# Default "inode"
#virtio_fs_dax_policy = "inode"

# Cache mode:
#
#  - none
#    Metadata, data, and pathname lookup are not cached in guest. They are
#    always fetched from host and any changes are immediately pushed to host.
#
#  - auto
#    Metadata and pathname lookup cache expires after a configured amount of
#    time (default is 1 second). Data is cached while the file is open (close
#    to open consistency).
#
#  - always
#    Metadata, data, and pathname lookup are cached in guest and never expire.
virtio_fs_cache = "always"

virtio_fs_extra_args = ["-o", "no_open,trace,cache_symlinks","--thread-pool-size=1"]

# Specifies driver to be used as hypervisor's root dev, could be virtio-blk or
# virtio-pmem. Default virtio-blk
#vm_rootfs_driver = "virtio-pmem"

# support pvdma
# pvdma="persistent"

# Block storage driver to be used for the hypervisor in case the container
# rootfs is backed by a block device. This is virtio-scsi, virtio-blk
# or nvdimm.
block_device_driver = "virtio-blk"

# Specifies cache-related options will be set to block devices or not.
# Default false
block_device_cache_set = true

# Specifies cache-related options for block devices.
# Denotes whether use of O_DIRECT (bypass the host page cache) is enabled.
# Default false
block_device_cache_direct = true

# Enable pre allocation of VM RAM, default false
# Enabling this will result in lower container density
# as all of the memory will be allocated and locked
# This is useful when you want to reserve all the memory
# upfront or in the cases where you want memory latencies
# to be very predictable
# Default false
#enable_mem_prealloc = true

# Enable huge pages for VM RAM, default false
# Enabling this will result in the VM memory
# being allocated using huge pages.
# This is useful when you want to use vhost-user network
# stacks within the container. This will automatically 
# result in memory pre allocation
#enable_hugepages = true
#
# Enable THP for VM RAM, default false
#thp = "shmem"

# Enable numa for hugepages, default false.
# If enable_hugepages is false, this value will be ignored.
# This feature only can be used in 96 pCPU machine.
# If enable this item, vm will always enable 96 vcpu.
#enable_numa = true

# Enable hotplug memory, default true
#enable_hotplug_memory = false

# Enable virtio-balloon, default false
# enable_balloon = true

# If mem_update_timeout_secs is set and bigger than 0, runtime will wait memory update
# status.  And this is the timeout seconds of wait.
# default 0.
# mem_update_timeout_secs = 10

# Default false
#enable_debug = true

#enable_debug = true

# Directory to store auto dumped vmcore. If dump_path is empty(default), vmcore will not be dumped.
# When setting dump_path, please ensure enough available space to store the vmcore.
# dump_path = "/tmp/"

# Dump format could be "ELF"/"Kdump-zlib"(default)/"Kdump-zstd". For ELF vmcore, the dump size is equal to memory size;
# for Kdump(strongly recommended) vmcore, the dump size is about 2 percent of the memory size.
# Notes: 1. Kdump-zlib format is not supported for aarch64. 2. To analyze Kdump-zstd format vmcore, you should use a crash compiled with zstd support.
# dump_format = "Kdump-zlib"

# Pvdump feature on Dragonball platform device is used for page exclusion and getting utsname in vmcore's header.
# If this feature is disabled, setting dump_level > 1 will be invalid, and vmcore will not contains utsname header.
# enable_pvdump = true

# The dump level [0-31] is used to guide page exclusion when dump format is Kdump-compressed.
# dump_level consists of five bits, so there are five base levels to specify the unnecessary page type.
# 1 : Exclude the pages filled with zero.
# 2 : Exclude the non-private cache pages.
# 4 : Exclude all cache pages.
# 8 : Exclude the user process data pages.
# 16 : Exclude the free pages.
# Default dump_level is 1. The larger dump_level is, the more pages will be excluded from vmcore.
# dump_level = 31

# The max io bandwidth for dump. Zero(default) to be no limit.
# dump_max_bps = 0

# The max iops for dump. Zero(default) to be no limit.
# dump_max_iops = 0

# Enable/disable auto dump when guest panic. Default is false.
# enable_auto_dump = false

# Max allowed auto dump time. Default is 4 * 60 * 60 = 4 hours.
# auto_dump_timeout = 14400

# Use virtio-rng device to supply entropy source to guest kernel.
enable_rng = true

# Use a non-default random number provider path for virtio-rng device.
# rng_backend_path = "/dev/urandom"

[hypervisor.dragonball.balloon_config]
# Enable VIRTIO_BALLOON_F_DEFLATE_ON_OOM for virtio-balloon, default false.
#f_deflate_on_oom = true

# Enable VIRTIO_BALLOON_F_REPORTING for virtio-balloon, default false.
#f_reporting = true

# Enable VIRTIO_BALLOON_F_CONT_PAGES for virtio-balloon, default false.
# Virtio-balloon with VIRTIO_BALLOON_F_CONT_PAGES will try to report
# continuous pages to incease resize speed.
#f_cont_pages = true

# Enable VIRTIO_BALLOON_F_ALLOC_RETRY for virtio-balloon, default false.
# Virtio-balloon with VIRTIO_BALLOON_F_ALLOC_RETRY will use
# __GFP_RETRY_MAYFAIL to allocate pages but not __GFP_NORETRY.
#f_alloc_retry = true

# Enable VIRTIO_BALLOON_F_FILL_H_OOM for virtio-balloon, default false.
# Virtio-balloon with VIRTIO_BALLOON_F_FILL_H_OOM will try to use
# virtio_balloon_fill_oom_notify handle the OOM when fill balloon.
#f_fill_h_oom = true

# Enable VIRTIO_BALLOON_F_FILL_A_OC for virtio-balloon, default false.
# Virtio-balloon with VIRTIO_BALLOON_F_FILL_A_OC will set
# sysctl_overcommit_memory to OVERCOMMIT_ALWAYS  when fill balloon.
# It can help prevent the task hang or crash due to memory allocation
# when fill the balloon.
#f_fill_a_oc = true

[hypervisor.dragonball.virtio_mem_config]
# When multi_region is true, virtio-mem device will multi-region allocate
# memory to handle the KVM vmalloc issue in the host.
# Then kata runtime will just insert single virtio-mem device.
# virtio-mem memory unplug and numa need this function.
# Default to false.
#multi_region = true

# when multi_region is true, this virtio-mem device will do asynchronous
# prealloc.
# The prealloc thread number is set to 1.
# Default to false.
#prealloc = true

# If init_mem_size > 0, hotplug memory_size memory except init_mem_size
# when sandbox start.
# Default value is 0.
#init_mem_size = 2048

[runtime]
# enable sandbox cgroup
sandbox_cgroup_only = false

# enable vcpu bind
vcpu_cgroup_support = false

# If enabled, the runtime will log additional debug messages to the
# system log
# (default: disabled)
#enable_debug = true

# If not provided, determined by enable_debug config.
# when enable_debug is true, the log level is Debug,
# or enable_debug is false, the log level is Info.
# If specified, use it first.
# Options:
# - trace
# - debug
# - info
# - warn
# - error
#
#log_level = "trace"

# The default is json format, if not provided.
# Options: "term"
#log_format = "term"

#
# Internetworking model
# Determines how the VM should be connected to the
# the container network interface
# Options:
#
#   - bridged
#     Uses a linux bridge to interconnect the container interface to
#     the VM. Works for most cases except macvlan and ipvlan.
#
#   - macvtap
#     Used when the Container network interface can be bridged using
#     macvtap.
#
#   - none
#     Used when customize network. Only creates a tap device. No veth pair.
#
#   - tcfilter
#     Uses tc filter rules to redirect traffic from the network interface
#     provided by plugin to a tap interface connected to the VM.
#
#   - default
#     Use the default network mode, which means:
#     For the physical network: passthrough it into vm
#     For the other network: using the tcfilter mode
internetworking_model="tcfilter"
#internetworking_model="none"

# agent reconnect timeout in milliseconds
agent_reconnect_timeout = 10000

# keep_alive used to monitor if agent is work in vm
keep_alive = true

# Enable builtin storage provided by sandbox, this storage will be used as the
# writable layer device for the pod, and shared by all containers in the pod.
# Default to false
#enable_sandbox_builtin_storage = true

# Define the template fs image of the writable layer device, which should
# contain ext4 filesystem, runtime will copy (do reflink copy if possible) the
# template file for current sandbox if enable_sandbox_builtin_storage is true.
#builtin_storage_template_image = ""

# Backend directory that contains copied images (from template image) for
# containers. If the directory is on the same filesystem with
# builtin_storage_template_image and the underlying fs supports reflink,
# reflink copy will be done. Note that the copied images will be removed after
# shim exits.
#builtin_storage_image_dir = ""

# When enable_sandbox_builtin_storage is true, this config tells agent to mount
# overlayfs with "volatile" mount option, which skips all sync operations on
# upperdir in guest.
# Default to false.
#use_volatile_builtin_storage = true

# When disable_mem_hotplug is true, container update will only ajust the container's
# memory cgroup and didn't do memory hotplug/unplug on guest.
# If the annotation of io.alibaba.pouch.vm.passthru.memory = false was passed with
# the container update call, it will override this config's setting and do the
# guest memory adjust.
#disable_mem_hotplug = true

# When builtin_storage_dev_only is true, runtime only attach builtin storage
# device to vmm, but don't let agent do mount operation, mount will be done by
# another service in VM.
# Default to false.
#builtin_storage_dev_only = true
#disable_new_netns = true

# When runtime update the memory of vm, call command in
# memory_update_hook_command if target size is bigger than
# current memory size.
# The script will be invoked with "update" argument, for example: /etc/init.d/kangaroo update
# memory_update_hook = "/etc/init.d/kangaroo"

# Enable audit service, when enable_audit is true, the audit-server will be started
# and the connection to Aegis will be initiated.
# (default: false)
#enable_audit = true
